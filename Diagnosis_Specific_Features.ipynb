{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c708861-5831-4d58-b829-1895c3226df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 0: SETUP\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6caca885-8c66-423b-abd3-34e960a609f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PART 1: Loading and Preprocessing Initial Data ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 1: DATA LOADING AND PREPROCESSING\n",
    "# ==============================================================================\n",
    "print(\"--- PART 1: Loading and Preprocessing Initial Data ---\")\n",
    "\n",
    "code_columns = {\n",
    "    'ICD9_DGNS_CD_1': str, 'ICD9_DGNS_CD_2': str, 'ICD9_DGNS_CD_3': str,\n",
    "    'ICD9_DGNS_CD_4': str, 'ICD9_DGNS_CD_5': str, 'ICD9_DGNS_CD_6': str,\n",
    "    'ICD9_DGNS_CD_7': str, 'ICD9_DGNS_CD_8': str, 'ICD9_DGNS_CD_9': str,\n",
    "    'ICD9_DGNS_CD_10': str, 'ADMTNG_ICD9_DGNS_CD': str, 'CLM_DRG_CD': str,\n",
    "    'ICD9_PRCDR_CD_1': str, 'ICD9_PRCDR_CD_2': str, 'ICD9_PRCDR_CD_3': str,\n",
    "    'ICD9_PRCDR_CD_4': str, 'ICD9_PRCDR_CD_5': str, 'ICD9_PRCDR_CD_6': str\n",
    "}\n",
    "\n",
    "beneficiary_2008 = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv\")\n",
    "beneficiary_2009 = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2009_Beneficiary_Summary_File_Sample_1.csv\")\n",
    "beneficiary_2010 = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2010_Beneficiary_Summary_File_Sample_1.csv\")\n",
    "\n",
    "chunk_size = 100000\n",
    "    \n",
    "inpatient_agg_list, inpatient_codes_list, inpatient_readmission_list = [], [], []\n",
    "inpatient_iterator = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\", dtype=code_columns, chunksize=chunk_size)\n",
    "for chunk in inpatient_iterator:\n",
    "    inpatient_agg_list.append(chunk.groupby('DESYNPUF_ID').agg(Inpatient_Claim_Count=('CLM_ID', 'count'), Total_Inpatient_Payments=('CLM_PMT_AMT', 'sum')))\n",
    "    inpatient_codes_list.append(chunk[['DESYNPUF_ID', 'ICD9_DGNS_CD_1']])\n",
    "    chunk['CLM_ADMSN_DT'] = pd.to_datetime(chunk['CLM_ADMSN_DT'], format='%Y%m%d')\n",
    "    chunk['CLM_THRU_DT'] = pd.to_datetime(chunk['CLM_THRU_DT'], format='%Y%m%d', errors='coerce')\n",
    "    inpatient_readmission_list.append(chunk)\n",
    "\n",
    "inpatient_agg = pd.concat(inpatient_agg_list).groupby(level=0).sum()\n",
    "inpatient_codes = pd.concat(inpatient_codes_list)\n",
    "inpatient_claims_raw = pd.concat(inpatient_readmission_list)\n",
    "    \n",
    "outpatient_agg_list, outpatient_codes_list = [], []\n",
    "outpatient_iterator = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv\", dtype=code_columns, engine='python', chunksize=chunk_size)\n",
    "for chunk in outpatient_iterator:\n",
    "    outpatient_agg_list.append(chunk.groupby('DESYNPUF_ID').agg(Outpatient_Claim_Count=('CLM_ID', 'count'), Total_Outpatient_Payments=('CLM_PMT_AMT', 'sum')))\n",
    "    outpatient_codes_list.append(chunk[['DESYNPUF_ID', 'ICD9_DGNS_CD_1']])\n",
    "        \n",
    "outpatient_agg = pd.concat(outpatient_agg_list).groupby(level=0).sum()\n",
    "outpatient_codes = pd.concat(outpatient_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2977a811-94ef-492e-bc5b-858cf0ba6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 2: Engineering Features and Merging Data ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 2: FEATURE ENGINEERING AND MERGING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 2: Engineering Features and Merging Data ---\")\n",
    "\n",
    "all_beneficiaries = pd.concat([beneficiary_2008, beneficiary_2009, beneficiary_2010], ignore_index=True)\n",
    "all_beneficiaries = all_beneficiaries.drop_duplicates(subset=['DESYNPUF_ID'], keep='last')\n",
    "    \n",
    "all_beneficiaries['BENE_BIRTH_DT'] = pd.to_datetime(all_beneficiaries['BENE_BIRTH_DT'], format='%m-%d-%Y')\n",
    "all_beneficiaries['BENE_DEATH_DT'] = pd.to_datetime(all_beneficiaries['BENE_DEATH_DT'], format='%m-%d-%Y', errors='coerce')\n",
    "reference_date = datetime(2010, 12, 31)\n",
    "all_beneficiaries['Age'] = ((reference_date - all_beneficiaries['BENE_BIRTH_DT']).dt.days / 365.25).astype(int)\n",
    "all_beneficiaries['Is_Dead'] = all_beneficiaries['BENE_DEATH_DT'].notna().astype(int)\n",
    "chronic_condition_cols = [col for col in all_beneficiaries.columns if col.startswith('SP_')]\n",
    "for col in chronic_condition_cols:\n",
    "    all_beneficiaries[col] = all_beneficiaries[col].replace(2, 0)\n",
    "all_beneficiaries['Chronic_Condition_Count'] = all_beneficiaries[chronic_condition_cols].sum(axis=1)\n",
    "    \n",
    "master_df = all_beneficiaries.merge(inpatient_agg, on='DESYNPUF_ID', how='left')\n",
    "master_df = master_df.merge(outpatient_agg, on='DESYNPUF_ID', how='left')\n",
    "claims_cols_to_fill = ['Inpatient_Claim_Count', 'Total_Inpatient_Payments', 'Outpatient_Claim_Count', 'Total_Outpatient_Payments']\n",
    "master_df[claims_cols_to_fill] = master_df[claims_cols_to_fill].fillna(0)\n",
    "\n",
    "inpatient_claims_raw = inpatient_claims_raw.sort_values(by=['DESYNPUF_ID', 'CLM_ADMSN_DT'])\n",
    "inpatient_claims_raw['Next_Admission_Date'] = inpatient_claims_raw.groupby('DESYNPUF_ID')['CLM_ADMSN_DT'].shift(-1)\n",
    "days_to_next_admission = (inpatient_claims_raw['Next_Admission_Date'] - inpatient_claims_raw['CLM_THRU_DT']).dt.days\n",
    "inpatient_claims_raw['Was_Readmitted_in_30_Days'] = (days_to_next_admission <= 30).astype(int)\n",
    "readmission_summary = inpatient_claims_raw.groupby('DESYNPUF_ID')['Was_Readmitted_in_30_Days'].max().reset_index()\n",
    "readmission_summary = readmission_summary.rename(columns={'Was_Readmitted_in_30_Days': 'Had_30Day_Readmission_Ever'})\n",
    "master_df_readmission = master_df.merge(readmission_summary, on='DESYNPUF_ID', how='left')\n",
    "master_df_readmission['Had_30Day_Readmission_Ever'] = master_df_readmission['Had_30Day_Readmission_Ever'].fillna(0)\n",
    "    \n",
    "all_codes = pd.concat([inpatient_codes, outpatient_codes], ignore_index=True)\n",
    "diagnosis_counts = all_codes.groupby('DESYNPUF_ID').size().reset_index(name='Total_Diagnosis_Count')\n",
    "unique_diagnosis_counts = all_codes.groupby('DESYNPUF_ID')['ICD9_DGNS_CD_1'].nunique().reset_index(name='Unique_Diagnosis_Count')\n",
    "master_df_enhanced = master_df_readmission.merge(diagnosis_counts, on='DESYNPUF_ID', how='left')\n",
    "master_df_enhanced = master_df_enhanced.merge(unique_diagnosis_counts, on='DESYNPUF_ID', how='left')\n",
    "master_df_enhanced[['Total_Diagnosis_Count', 'Unique_Diagnosis_Count']] = master_df_enhanced[['Total_Diagnosis_Count', 'Unique_Diagnosis_Count']].fillna(0)\n",
    "categorical_cols = ['BENE_SEX_IDENT_CD', 'BENE_RACE_CD']\n",
    "master_df_enhanced = pd.get_dummies(master_df_enhanced, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "try:\n",
    "    drug_exposure = pd.read_excel(\"D:/Jupyter/HealthArk_data/drug_exposure.xlsx\")\n",
    "    person_mapping = pd.read_excel(\"D:/Jupyter/HealthArk_data/person.xlsx\")\n",
    "    person_id_map = person_mapping[['PERSON_ID', 'PERSON_SOURCE_VALUE']].rename(columns={'PERSON_SOURCE_VALUE': 'DESYNPUF_ID'})\n",
    "    drug_exposure = drug_exposure.merge(person_id_map, on='PERSON_ID', how='left')\n",
    "    \n",
    "    if 'DESYNPUF_ID' in drug_exposure.columns:\n",
    "        drug_counts = drug_exposure.groupby('DESYNPUF_ID').size().reset_index(name='Total_Drug_Count')\n",
    "        unique_drug_counts = drug_exposure.groupby('DESYNPUF_ID')['DRUG_CONCEPT_ID'].nunique().reset_index(name='Unique_Drug_Count')\n",
    "        avg_days_supply = drug_exposure.groupby('DESYNPUF_ID')['DAYS_SUPPLY'].mean().reset_index(name='Avg_Days_Supply')\n",
    "        master_df_final = master_df_enhanced.merge(drug_counts, on='DESYNPUF_ID', how='left')\n",
    "        master_df_final = master_df_final.merge(unique_drug_counts, on='DESYNPUF_ID', how='left')\n",
    "        master_df_final = master_df_final.merge(avg_days_supply, on='DESYNPUF_ID', how='left')\n",
    "        drug_feature_cols = ['Total_Drug_Count', 'Unique_Drug_Count', 'Avg_Days_Supply']\n",
    "        master_df_final[drug_feature_cols] = master_df_final[drug_feature_cols].fillna(0)\n",
    "except FileNotFoundError:\n",
    "    print(\"Drug or Person files not found. Skipping drug features.\")\n",
    "    master_df_final = master_df_enhanced.copy()\n",
    "    master_df_final[['Total_Drug_Count', 'Unique_Drug_Count', 'Avg_Days_Supply']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5507af0e-38d2-4cf4-96f1-a09f4b4a41bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 3: Engineering Diagnosis-Specific Features (Memory-Safe) ---\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv for Has_Heart_Failure_Dx...\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv for Has_Heart_Failure_Dx...\n",
      "Created feature: Has_Heart_Failure_Dx - Found in 25046 patients.\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv for Has_Kidney_Disease_Dx...\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv for Has_Kidney_Disease_Dx...\n",
      "Created feature: Has_Kidney_Disease_Dx - Found in 19702 patients.\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv for Has_Sepsis_Dx...\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv for Has_Sepsis_Dx...\n",
      "Created feature: Has_Sepsis_Dx - Found in 0 patients.\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv for Has_Pneumonia_Dx...\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv for Has_Pneumonia_Dx...\n",
      "Created feature: Has_Pneumonia_Dx - Found in 9397 patients.\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv for Has_COPD_Dx...\n",
      "Processing D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv for Has_COPD_Dx...\n",
      "Created feature: Has_COPD_Dx - Found in 24905 patients.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 3: ADVANCED FEATURE ENGINEERING (DIAGNOSIS-SPECIFIC FLAGS) - MEMORY SAFE\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 3: Engineering Diagnosis-Specific Features (Memory-Safe) ---\")\n",
    "\n",
    "# Define ICD-9 code prefixes for key conditions\n",
    "diag_code_map = {\n",
    "    'Has_Heart_Failure_Dx': ['428'],\n",
    "    'Has_Kidney_Disease_Dx': ['585', '586'],\n",
    "    'Has_Sepsis_Dx': ['038'],\n",
    "    'Has_Pneumonia_Dx': ['480', '481', '482', '483', '485', '486'],\n",
    "    'Has_COPD_Dx': ['491', '492', '496']\n",
    "}\n",
    "\n",
    "# Identify all diagnosis code columns\n",
    "diag_cols = [f'ICD9_DGNS_CD_{i}' for i in range(1, 11)]\n",
    "\n",
    "# List of claims files to process\n",
    "claims_files = [\n",
    "    \"D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\",\n",
    "    \"D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv\"\n",
    "]\n",
    "\n",
    "# Create a feature for each condition\n",
    "for feature_name, codes in diag_code_map.items():\n",
    "    \n",
    "    all_patients_with_diagnosis = set()\n",
    "    \n",
    "    # Process each claims file in chunks\n",
    "    for file in claims_files:\n",
    "        print(f\"Processing {file} for {feature_name}...\")\n",
    "        engine_type = 'python' if 'Outpatient' in file else 'c'\n",
    "        iterator = pd.read_csv(file, dtype=code_columns, chunksize=100000, engine=engine_type)\n",
    "        \n",
    "        for chunk in iterator:\n",
    "            # Check if any of the diagnosis columns contain any of the specified codes\n",
    "            has_diagnosis = chunk[diag_cols].apply(\n",
    "                lambda row: any(str(val).startswith(tuple(codes)) for val in row),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Get the unique patient IDs from this chunk who have the diagnosis\n",
    "            patients_in_chunk = chunk.loc[has_diagnosis, 'DESYNPUF_ID'].unique()\n",
    "            \n",
    "            # Add them to our master set of patients\n",
    "            all_patients_with_diagnosis.update(patients_in_chunk)\n",
    "\n",
    "    # Create the new feature in our final dataframe\n",
    "    master_df_final[feature_name] = master_df_final['DESYNPUF_ID'].isin(all_patients_with_diagnosis).astype(int)\n",
    "    print(f\"Created feature: {feature_name} - Found in {len(all_patients_with_diagnosis)} patients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca9c7a4-78b5-4802-9db7-2f0709e8b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 4: Training and Evaluating the Ultimate Model ---\n",
      "\n",
      "--- Final Model Performance Evaluation (with Diagnosis-Specific Features) ---\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.94      0.97     22021\n",
      "         1.0       0.47      0.96      0.63      1250\n",
      "\n",
      "    accuracy                           0.94     23271\n",
      "   macro avg       0.73      0.95      0.80     23271\n",
      "weighted avg       0.97      0.94      0.95     23271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 4: FINAL MODEL TRAINING AND EVALUATION\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 4: Training and Evaluating the Ultimate Model ---\")\n",
    "\n",
    "y = master_df_final['Had_30Day_Readmission_Ever']\n",
    "features_to_drop = ['DESYNPUF_ID', 'BENE_BIRTH_DT', 'BENE_DEATH_DT', 'Had_30Day_Readmission_Ever']\n",
    "X = master_df_final.drop(columns=features_to_drop)\n",
    "X = X.select_dtypes(include=['number'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# Use the best parameters we found from our previous tuning\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "ultimate_model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "ultimate_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ultimate_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Final Model Performance Evaluation (with Diagnosis-Specific Features) ---\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76808d-439a-4927-bbb0-5bce6b057a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
